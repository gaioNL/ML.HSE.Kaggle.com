{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update 20.12.2017.3 - introduced early stopping for LightGBM & updated LightGBM params\n",
    "\n",
    "Update 21.12.2017 - Introduced stacking & ensemble\n",
    "\n",
    "Udate 02.01.2018 - restarted working on EDA\n",
    "\n",
    "Update 09.01.2018 - working on feature generation\n",
    "\n",
    "Update 15.01.2018 - working on feature hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project: predict future sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n",
    "In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n",
    "\n",
    "We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../readonly/final_project_data/'\n",
    "\n",
    "sales    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'))\n",
    "items           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\n",
    "shops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "train           = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'), compression='gzip')\n",
    "test           = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the loaded dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('sales shape %s' % np.str(sales.shape))\n",
    "print ('items shape %s' % np.str(items.shape))\n",
    "print ('item_categories shape %s' % np.str(item_categories.shape))\n",
    "print ('shops shape %s' % np.str(shops.shape))\n",
    "print ('train shape %s' % np.str(train.shape))\n",
    "print ('test shape %s' % np.str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse data heads to get an idea of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. The negative values in item_cnt_day are returned items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I join the data to get the consolidated base data frame\n",
    "\n",
    "1st I add category description to the items df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items_merge = pd.merge(left = items, right = item_categories , left_on = 'item_category_id', right_on = 'item_category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (items_merge.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I add the category to the items sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(left = sales,right = items_merge, left_on ='item_id', right_on = 'item_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sales_merge.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the time span of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = pd.to_datetime(sales_merge['date'],format='%d.%m.%Y')\n",
    "print ('Sale from %s to %s' % (str(dates_df.min()),str(dates_df.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note. 2 years & 10 months of sales data. I need to predict the sales in November 2015\n",
    "\n",
    "### Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values?\n",
    "sales_merge.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data, all columns have been populated\n",
    "\n",
    "I visualize how prices are distributed to understand whether there are some dummy values & outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.boxplot(column = 'item_price')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sales_merge.item_category_id,sales_merge.item_price)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I did is the following:\n",
    "* 1st I have checked the prices alone & noticed that most prices are < 100000. \n",
    "* Then I have checked the distribution of prices over the item categories & observed that the price > 100000 should be an outlier\n",
    "\n",
    "Now I quantify the items that are extremely highly priced to confirm I can remove them from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge[sales_merge.item_price > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge[sales_merge.item_category_id == 75]['item_price'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note. The high price is probably a typo. I updated the data with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_merge.at[2163826, 'item_price'] = 1859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.iloc[2163826]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaio Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales.at[2163826, 'item_price'] = 1859\n",
    "sales = sales[sales.item_cnt_day<=1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to visualize the sales over the train period by grouping the items sold by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggrMonth = sales_merge.groupby(['date_block_num'])[['item_cnt_day']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggrMonth.item_cnt_day)\n",
    "plt.title (\"Items Sold/Month\")\n",
    "plt.xlabel(\"month\") \n",
    "plt.ylabel(\"items sold\") \n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES: \n",
    "\n",
    "* There seems to be some seasonality in the sales, with an indication of simmetry.\n",
    "* We can notice a sharp increase followed by a sharp decrease around the spikes. This can be exploited by adding lagged features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 1st the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_cols = list(sales)\n",
    "df_test_append = pd.DataFrame(index=test.index, columns=all_data_cols)\n",
    "df_test_append = df_test_append.fillna(0)\n",
    "df_test_append['shop_id'] = test['shop_id']\n",
    "df_test_append['item_id'] = test['item_id']\n",
    "df_test_append['date_block_num'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_append.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.concat((sales, df_test_append)).reset_index(drop=True)\n",
    "print(\"all_data size is : {}\".format(sales.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DF with all combination of month, shop & item\n",
    "\n",
    "This is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n",
    "    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "#turn the grid into pandas dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_merge = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\n",
    "sales_merge = pd.merge(grid,sales_merge,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "# adding the category id too\n",
    "sales_merge = pd.merge(sales_merge,items,on=['item_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the mean encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for type_id in ['item_id','shop_id','item_category_id']:\n",
    "    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n",
    "\n",
    "        mean_df = sales_merge.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n",
    "        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n",
    "\n",
    "        sales_merge = pd.merge(sales_merge,mean_df,on=['date_block_num',type_id],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having create the grid, I add the lags up to 1 year before to leverage the sales seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rename = list(sales_merge.columns[7:]) + ['item_cnt_day']\n",
    "print (cols_to_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "sales_merge = downcast_dtypes(sales_merge)\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    print(\"passa %s\\n\" % str(month_shift))\n",
    "    train_shift = sales_merge[index_cols + cols_to_rename].copy()\n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "    sales_merge = pd.merge(sales_merge, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    del train_shift\n",
    "    \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat in sales_merge.columns:\n",
    "    if 'item_cnt' in feat:\n",
    "        sales_merge[feat]=sales_merge[feat].fillna(0)\n",
    "    elif 'item_price' in feat:\n",
    "        sales_merge[feat]=sales_merge[feat].fillna(sales_merge[feat].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the columns that I am not going to use for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_cols = cols_to_rename + ['item_name','item_price']\n",
    "print (to_drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get just recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_merge = sales_merge[sales_merge['date_block_num']>12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will train on the 1st 32 months & validate on the last month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('1st month: %s , last month: %s' % (str(sales_merge['date_block_num'].min()),str(sales_merge['date_block_num'].max())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = sales_merge.loc[sales_merge['date_block_num']<33]['item_cnt_day']\n",
    "y_val =  sales_merge.loc[sales_merge['date_block_num'] == 33]['item_cnt_day']\n",
    "\n",
    "X_train = sales_merge[sales_merge['date_block_num']<33].drop(to_drop_cols, axis=1)\n",
    "X_val =  sales_merge[sales_merge['date_block_num']==33].drop(to_drop_cols, axis=1)\n",
    "X_test = sales_merge[sales_merge['date_block_num']==34].drop(to_drop_cols, axis=1)\n",
    "\n",
    "X_final_train = sales_merge[sales_merge['date_block_num']<34].drop(to_drop_cols, axis=1)\n",
    "y_final_train = sales_merge.loc[sales_merge['date_block_num']<34]['item_cnt_day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip the training target in the range [0,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=np.clip(y_train,0, 40)\n",
    "y_val=np.clip(y_val,0, 40)\n",
    "y_final_train = np.clip(y_final_train,0, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function - RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(X,y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select most important features with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "#GAIO Update 20.12.2017.3 - updated lgb params\n",
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0,\n",
    "                'early_stopping_rounds' :1,\n",
    "                'eval_metric':'rmse'\n",
    "              }\n",
    "\n",
    "#GAIO Update 20.12.2017.3 - introduced early stopping having noticed the model overfitting\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "model_lgb = lgb.train(lgb_params,                     \n",
    "                      lgb_train,\n",
    "                      num_boost_round=500,\n",
    "                      valid_sets=lgb_eval,\n",
    "                      early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_lgb,importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune LightGBM with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM uses the leaf-wise tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can convenge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters.\n",
    "\n",
    "To get good results using a leaf-wise tree, these are some important parameters:\n",
    "\n",
    "* num_leaves. This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to convert from depth-wise tree. However, this simple conversion is not good in practice. The reason is, when number of leaves are the same, the leaf-wise tree is much deeper than depth-wise tree. As a result, it may be over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=6 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise. Actually, the concept depth can be forgotten in leaf-wise tree, since it doesn't have a correct mapping from leaves to depth.\n",
    "* min_data_in_leaf. This is a very important parameter to deal with over-fitting in leaf-wise tree. Its value depends on the number of training data and num_leaves. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n",
    "* max_depth. You also can use max_depth to limit the tree depth explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "def objective(space):\n",
    "    print(space)\n",
    "    \n",
    "    lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'max_depth' : int(space['max_depth']),\n",
    "               'num_leaves': int(space['num_leaves']),\n",
    "               'min_data_in_leaf': int(space['min_data_in_leaf']), \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': int(space['bagging_seed']), \n",
    "               'bagging_freq':1,\n",
    "               'verbose':0,\n",
    "                'early_stopping_rounds' :1,\n",
    "                'eval_metric':'rmse'\n",
    "              }\n",
    "    opt_lgb = lgb.train(lgb_params,                     \n",
    "                      lgb_train,\n",
    "                      num_boost_round=500,\n",
    "                      valid_sets=lgb_eval,\n",
    "                      early_stopping_rounds=5)\n",
    "    \n",
    "    pred = opt_lgb.predict(X_val)\n",
    "    mse_scr = mean_squared_error(y_val, pred)\n",
    "    print (\"SCORE:\", np.sqrt(mse_scr))\n",
    "    return {'loss':mse_scr, 'status': STATUS_OK }\n",
    "\n",
    "space ={    \n",
    "    'max_depth': hp.quniform(\"x_max_depth\", 4, 16, 1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 10, 140, 10),\n",
    "    'bagging_seed': hp.quniform('bagging_seed', 10, 140, 10),\n",
    "    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 20, 140, 10)\n",
    "    }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,            \n",
    "            trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with optimal parameters & predict the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "# Get the values of the optimal parameters\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# Fit the model with the optimal hyperparamters\n",
    "lgb_final_train = lgb.Dataset(X_final_train, y_final_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lgb = lgb.train(best_params,                     \n",
    "                      lgb_final_train,\n",
    "                      num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model_lgb.predict(X_train,num_iteration=model_lgb.best_iteration)\n",
    "rmse_train = rmse(y_train, train_preds)\n",
    "\n",
    "val_preds = model_lgb.predict(X_val,num_iteration=model_lgb.best_iteration)\n",
    "rmse_val = rmse(y_val, val_preds)\n",
    "\n",
    "print('Train RMSE is %f' % rmse_train)\n",
    "print('Validation RMSE is %f' % rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_lgb = model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clip the target values in the range 0-20\n",
    "out_df = pd.DataFrame({'ID': test.ID, 'item_cnt_month': np.clip(pred_lgb,0,20)})\n",
    "# you could use any filename. We choose submission here\n",
    "out_df.to_csv('predict_future_prices_20180115.v0.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = bst.predict(data, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Level Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I train Linear Regression & LightGBM on training & predict on Training & Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "train_preds = lr.predict(X_train.values)\n",
    "rmse_lr_train = rmse(y_train, train_preds)\n",
    "\n",
    "pred_lr_val = lr.predict(X_val.values)\n",
    "rmse_lr_val = rmse(y_val, pred_lr_val)\n",
    "\n",
    "print('Train RMSE is %f' % rmse_lr_train)\n",
    "print('Validation RMSE is %f' % rmse_lr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model_lgb.predict(X_train)\n",
    "rmse_train = rmse(y_train, train_preds)\n",
    "\n",
    "val_preds = model_lgb.predict(X_val)\n",
    "rmse_val = rmse(y_val, val_preds)\n",
    "\n",
    "print('Train R-squared is %f' % rmse_train)\n",
    "print('Validation R-squared is %f' % rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I predict on the test set & concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test.values)\n",
    "pred_lgb = model_lgb.predict(X_test)\n",
    "X_test_level2 = np.c_[pred_lr, pred_lgb] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIO 13-01-18 - Gebruik lgb voorspel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test = model_lgb.predict(X_test)\n",
    "#clip the target values in the range 0-20\n",
    "out_df = pd.DataFrame({'ID': test.ID, 'item_cnt_month': np.clip(X_test_level2[:,1],0,20)})\n",
    "# you could use any filename. We choose submission here\n",
    "out_df.to_csv('predict_future_prices_20180112.v0.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use duration T equal to month and M=15.\n",
    "We get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = sales_merge['date_block_num']\n",
    "dates_train = dates[dates <  34]\n",
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    X_train_l1 = X_train.loc[dates_train < cur_block_num]\n",
    "    y_train_l1 = y_train[dates_train < cur_block_num]\n",
    "    X_test_l1 = X_train.loc[dates_train == cur_block_num]\n",
    "        \n",
    "    lr.fit(X_train_l1.values, y_train_l1)\n",
    "    pred_lr = lr.predict(X_test_l1.values)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train_l1, label=y_train_l1), 100)\n",
    "    pred_lgb = model.predict(X_test_l1)\n",
    "    \n",
    "    X_train_level2[dates_train_level2 == cur_block_num] = np.c_[pred_lr, pred_lgb] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "best_alpha = -1\n",
    "rmse_train_simple_mix = -1000\n",
    "for current_alpha in alphas_to_try:\n",
    "    mix = current_alpha * X_train_level2[:,0] + (1 - current_alpha) * X_train_level2[:,1]\n",
    "    current_rmse = rmse(y_train_level2, mix)\n",
    "    if current_rmse > rmse_train_simple_mix:\n",
    "        best_alpha = current_alpha\n",
    "        rmse_train_simple_mix = current_rmse\n",
    "        \n",
    "print('Best alpha: %f; Corresponding RMSE score on train: %f' % (best_alpha, rmse_train_simple_mix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test = model_lgb.predict(X_test)\n",
    "#clip the target values in the range 0-20\n",
    "out_df = pd.DataFrame({'ID': test.ID, 'item_cnt_month': np.clip(test_preds,0,20)})\n",
    "# you could use any filename. We choose submission here\n",
    "out_df.to_csv('predict_future_prices_20180112.v0.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hw_version": "1.0.0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "0b671e69c2224f37b45e5a1927794395": {
     "views": [
      {
       "cell_index": 51
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
