{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict future sales\n",
    "In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n",
    "\n",
    "We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import save_npz, load_npz, hstack, vstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../readonly/final_project_data/'\n",
    "\n",
    "sales    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'))\n",
    "items           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\n",
    "shops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "train           = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'), compression='gzip')\n",
    "test           = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('sales shape %s' % np.str(sales.shape))\n",
    "print ('items shape %s' % np.str(items.shape))\n",
    "print ('item_categories shape %s' % np.str(item_categories.shape))\n",
    "print ('shops shape %s' % np.str(shops.shape))\n",
    "print ('train shape %s' % np.str(train.shape))\n",
    "print ('test shape %s' % np.str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I give a 1st look at the data.\n",
    "Sales & Train have the same shape. Are the same df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.equals(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I need to predict the shop sales, which in this case means predicting the sales of the combination of shop & product, not just the total shop sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st I add the descriptions to shops & categories in the sales df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items_merge = pd.merge(left = items, right = item_categories , left_on = 'item_category_id', right_on = 'item_category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(left = sales,right = shops, left_on ='shop_id', right_on = 'shop_id' )\n",
    "sales_merge = pd.merge(left = sales_merge,right = items_merge, left_on ='item_id', right_on = 'item_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i check the types of the cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily sales is float. Are there any partial sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_merge.item_cnt_day%1 != 0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any NaNs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there zero sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge[sales_merge['item_cnt_day']==0]['item_cnt_day'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no occurrences of zero sales in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all cells have been populated with some values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does each item belong just to one category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items_merge.groupby(['item_name','item_category_name']).nunique()) == len(items_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look how train & test are constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(sales_merge.shop_id) - set(test.shop_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All shops in the test set are also in the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(test.item_id) - set(sales_merge.item_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merda! 363 items have been placed in the test set but they have never been observed in the train set... \n",
    "this can be an hint that the test set has been artificially constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test.groupby(['shop_id','item_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(test.item_id)) * len(set(test.shop_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypo confirmed. The test set has been made by combining a set of items with a set of shops.\n",
    "I can expect that there will be zero sales for many shop-item combinations.\n",
    "\n",
    "I will have to add the zero lines with all combinations of shop & item in the train set too to align it with test set. \n",
    "Why? Because any model trained on the original train set would never predict zero sales as there are no zero sales in the training set.\n",
    "The accuracy on the test set will then be very low as in this set zero sales are expected.\n",
    "\n",
    "Now what I would like to do is try to understand whether there was a logic in the selection of the items/shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_items_test = set(test.item_id) - set(sales_merge.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_merge.loc[add_items_test].sort_values(['item_category_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_merge.loc[add_items_test].sort_values(['item_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Items belong to one of the following categories: \n",
    "console games, PC games, console/PC accessories, Movies, Music, eBooks & SW & merchandise related to games/video\n",
    "\n",
    "* I will need to extract text feature to help the model exploit the info in the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_merge.loc[add_items_test].sort_values(['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items are ordered by their 1st letter & there seem to be 2 main groups\n",
    "\n",
    "TO DO: Count Items by 1st letter & category & investigate further\n",
    "\n",
    "Now let's start visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_eda = sales_merge.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Add details about weeks, days of week etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sales_eda['date'] = pd.to_datetime(sales_eda['date'],format='%d.%m.%Y')\n",
    "sales_eda['nr_dow'] = sales_eda.date.dt.weekday\n",
    "sales_eda['month'] = sales_eda.date.dt.month\n",
    "sales_eda['year'] = sales_eda.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_eda.sort_values(['date'], inplace=True)\n",
    "sales_eda.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eda['date_block_num'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.plot raw sales over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby('date').item_cnt_day.sum().plot(ax=ax)\n",
    "ax = plt.subplot(212)\n",
    "sales_eda.groupby('date_block_num').item_cnt_day.sum().plot(ax=ax)\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(34))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* spikes in Dec = shopping for Xmas, though this is celebrated in Jan\n",
    "* there seems to be trend over the years\n",
    "* total sales are going down over the years\n",
    "\n",
    "Now I check the dates span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = pd.to_datetime(sales_eda['date'],format='%d.%m.%Y')\n",
    "print ('Sale from %s to %s' % (str(dates_df.min()),str(dates_df.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to predict the sales in November 2015.\n",
    "\n",
    "Though I know that the test set has been artificially generated, \n",
    "\n",
    "I am going to plot Nov 2013 & Nov 2014 to check whether they can be good candidates as valitation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda[sales_eda['date_block_num'] == 10].groupby('date').item_cnt_day.sum().plot(ax=ax)\n",
    "ax = plt.subplot(212)\n",
    "sales_eda[sales_eda['date_block_num'] == 22].groupby('date').item_cnt_day.sum().plot(ax=ax)\n",
    "plt.grid(True)\n",
    "#plt.xticks(np.arange(31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mah...not really helpful\n",
    "\n",
    "Now I am going to plot the sales per month across the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby(['year', 'month']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "plt.xlim(1, 12)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.title('Sales per month at different years')\n",
    "plt.ylabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sales are decreasing\n",
    "* We can see the trend in the years\n",
    "\n",
    "Now I look for the sales during the weeksdays over the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda[sales_eda['item_cnt_day'] >= 0].groupby(['year', 'nr_dow']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "plt.grid(True)\n",
    "ax = plt.subplot(212)\n",
    "sales_eda[sales_eda['item_cnt_day'] < 0].groupby(['year', 'nr_dow']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most sales happen on saturdays\n",
    "* Most returns on Mondays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby(['year','shop_id']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "#plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting - shops open & close over time. I need then to add the zero sales for these cases.\n",
    "\n",
    "We can see that it seems that shops maintain the same sales performances over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby(['date_block_num','shop_id']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "#plt.grid(True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it looks like shops show regularity in performance over time. To be captured in feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby(['year','item_category_id']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "#plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a bunch of shops & categories with very high sales\n",
    "\n",
    "Now I look for outliers.\n",
    "Shop & Category sales spikes are consistent over the years, I will then not look into them further for the time being.\n",
    "I check the product sales per product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "sales_eda.groupby(['date_block_num','item_category_id']).item_cnt_day.sum().unstack(level=0).plot(ax=ax)\n",
    "#plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for categories - there is regularity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "item_sales = sales_eda.groupby(['year','item_id']).item_cnt_day.sum()\n",
    "item_sales.unstack(level=0).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's look at the product with the highest sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(211)\n",
    "item_sales = sales_eda.groupby(['date_block_num','item_id']).item_cnt_day.sum()\n",
    "item_sales[item_sales.values > 100].unstack(level=0).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_prod = item_sales[item_sales.values > 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_merge[items_merge.item_id.isin(list(top_prod.index.get_level_values(level=1)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is a shopping bag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate data\n",
    "\n",
    "Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggr_train(sales):\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "    for block_num in sales['date_block_num'].unique():\n",
    "        cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n",
    "        cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    " \n",
    "    #turn the grid into pandas dataframe\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "    \n",
    "    #get aggregated values for (shop_id, item_id, month)\n",
    "    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "\n",
    "    #fix column names\n",
    "    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "    #join aggregated data to the grid\n",
    "    all_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n",
    "    #sort the data\n",
    "    #all_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True) \n",
    "    all_data.reset_index(inplace=True, drop=True)\n",
    "    all_data.target = all_data.target.astype(np.float32)\n",
    "       \n",
    "    return all_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = aggr_train(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add item category ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['item_category_id'] = train_df['item_id'].map(items['item_category_id']).astype(np.int32)\n",
    "test['item_category_id'] = test['item_id'].map(items['item_category_id']).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect();\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "test_index_cols = ['shop_id', 'item_id']\n",
    "cols_to_rename = ['target']\n",
    "\n",
    "shift_range = [1, 2, 3, 6, 12]\n",
    "\n",
    "train_df = downcast_dtypes(train_df)\n",
    "test = downcast_dtypes(test)\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    print(\"passa %s\\n\" % str(month_shift))\n",
    "    train_shift = train_df[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "    \n",
    "    train_df = pd.merge(train_df, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    # Test\n",
    "    test_month_shift = 34 - month_shift\n",
    "    test_shift = train_df.loc[train_df.date_block_num == test_month_shift, test_index_cols + cols_to_rename].copy()\n",
    "    test_shift = test_shift.rename(columns=foo)\n",
    "    \n",
    "    test = pd.merge(test, test_shift, on=test_index_cols, how='left').fillna(0)\n",
    "\n",
    "lagged_features = [col for col in train.columns if 'lag' in col]\n",
    "del train_shift, test_shift   \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tfidfs\n",
    "train_tf_idf_features = {}\n",
    "test_tf_idf_features = {}\n",
    "col_vals = [shops.shop_name, items.item_name]\n",
    "col_names = ['shop_id', 'item_id']\n",
    "\n",
    "fName = \"shop_id_tf_idf_train.npz\"\n",
    "\n",
    "if os.path.exists(fName):\n",
    "    print('Loading text features...')\n",
    "    for name in col_names:\n",
    "        train_tf_idf_features[name] = load_npz(name + '_tf_idf_train.npz')\n",
    "        test_tf_idf_features[name] = load_npz(name + '_tf_idf_test.npz')\n",
    "else:\n",
    "    print('Generate text features...')\n",
    "    #load russian + relevant english stopwords\n",
    "    stop_words_ru= ['для','тц', 'тк', 'трк', 'трц', 'ii', 'ул', 'пав','the', 'для', 'of', 'на']\n",
    "    \n",
    "    for name, text in zip(col_names, col_vals):\n",
    "        print(f'Tfidf from feature {name}')\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=30, stop_words=stop_words_ru)\n",
    "        tf_idf_feats = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "        means = np.array(tf_idf_feats.mean(axis=0)).squeeze()\n",
    "        argsort = means.argsort()\n",
    "        print('Top frequency words:')\n",
    "        print(np.array(tfidf_vectorizer.get_feature_names())[argsort[::-1][:30]])\n",
    "        print()\n",
    "\n",
    "        # Create and save\n",
    "        print('Transform data')\n",
    "        sparse_text_train = tfidf_vectorizer.transform(train_df[name].map(text))\n",
    "        sparse_text_test = tfidf_vectorizer.transform(test[name].map(text))\n",
    "        train_tf_idf_features[name] = sparse_text_train\n",
    "        test_tf_idf_features[name] = sparse_text_test\n",
    "        \n",
    "        print('Save to file')\n",
    "        save_npz(name + '_tf_idf_train', sparse_text_train)\n",
    "        save_npz(name + '_tf_idf_test', sparse_text_test)\n",
    "        print()\n",
    "print (\"...done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf_features['item_id'].toarray().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_mean_enc(tr, test, col,global_mean):\n",
    "    new_col_name = col + '_enc'\n",
    "    tr[new_col_name] = np.nan\n",
    "\n",
    "    cumsum = tr.groupby(col)['target'].cumsum() - tr['target']\n",
    "    cumcount = tr.groupby(col)['target'].cumcount()\n",
    "\n",
    "    expanding_mean = pd.DataFrame({'expand_mean': cumsum / cumcount,\n",
    "                                   'date_block_num': tr.date_block_num,\n",
    "                                   col: tr[col]})\n",
    "    for block_num in np.unique(tr.date_block_num)[1:]:\n",
    "        cur_mask = tr.date_block_num == block_num\n",
    "        prev_mask = tr.date_block_num <= block_num - 1\n",
    "        \n",
    "        mapping = expanding_mean[prev_mask].groupby(col).expand_mean.last()\n",
    "        tr.loc[cur_mask, new_col_name] = tr.loc[cur_mask, col].map(mapping)\n",
    "        \n",
    "    \n",
    "    # Fill train with last seen values from train\n",
    "    prev_mask = tr.date_block_num <= tr.date_block_num.max()\n",
    "    mapping = expanding_mean[prev_mask].groupby(col).expand_mean.last()\n",
    "    test[new_col_name] = test[col].map(mapping)\n",
    "    \n",
    "    # Downcast\n",
    "    tr[new_col_name] = tr[new_col_name].astype(np.float32)\n",
    "    test[new_col_name] = test[new_col_name].astype(np.float32)\n",
    "    \n",
    "    # Fill NaNs\n",
    "    tr[new_col_name].fillna(global_mean, inplace=True) \n",
    "    tr[new_col_name].replace(np.inf, global_mean, inplace=True)\n",
    "\n",
    "    test[new_col_name].fillna(global_mean, inplace=True) \n",
    "    test[new_col_name].replace(np.inf, global_mean, inplace=True)\n",
    "    return tr,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_4_mean = ['item_id','shop_id','item_category_id']\n",
    "global_mean = train_df.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in col_4_mean:\n",
    "    train_df, test = expand_mean_enc(train_df, test, col,global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[np.random.choice(train_df.index,10,replace=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_month = 6\n",
    "train_sel_mask = train_df.date_block_num > start_month\n",
    "train_df = train_df[train_sel_mask]\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "for name in sorted(train_tf_idf_features):\n",
    "    train_tf_idf_features[name]=train_tf_idf_features[name][train_sel_mask.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_target(counts):\n",
    "    return np.clip(counts, 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set\n",
    "I have noticed that the spikes in the data have a 3 month span (start - spike - end). I will then use the last 2 months as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('1st month: %s , last month: %s' % (str(train_df['date_block_num'].min()),str(train_df['date_block_num'].max())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mask = train_df.date_block_num < 32\n",
    "val_mask = train_df.date_block_num >= 32\n",
    "\n",
    "x_train= train_df.loc[train_mask].copy()\n",
    "x_val = train_df.loc[val_mask].copy()\n",
    "\n",
    "y_train = x_train['target']\n",
    "y_val = x_val['target']\n",
    "\n",
    "#drop target column\n",
    "to_drop_cols = ['target']\n",
    "x_train.drop(to_drop_cols, axis=1, inplace=True)\n",
    "x_val.drop(to_drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "1) Tuning with Hyperopt\n",
    "\n",
    "2) Plot the results\n",
    "\n",
    "3) Plot the features by importance\n",
    "\n",
    "4) Retrain with tuned parameters and most important features\n",
    "\n",
    "### Tuning \n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    " \n",
    "https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "https://www.quora.com/How-do-I-tune-hyperparameters-like-eta-num_rounds-max_depth-for-xgboost\n",
    "\n",
    "http://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/\n",
    "\n",
    "https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html\n",
    "\n",
    "1) 1st Tune max_depth' , 'min_child_weight', 'subsample', 'colsample_bytree' \n",
    "\n",
    "2) Tune eta - learning rate\n",
    "\n",
    "3) Find the optimal n_estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    print(space)\n",
    "    \n",
    "    xgb_params = {\n",
    "        'colsample_bytree' : space['colsample_bytree'],\n",
    "        'learning_rate' : .3,\n",
    "        'max_depth' : int(space['max_depth']),\n",
    "        'min_child_weight' : space['min_child_weight'],\n",
    "        'subsample' : space['subsample'],\n",
    "        'gamma' : space['gamma'],\n",
    "        'reg_lambda' : space['reg_lambda']\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBRegressor(**xgb_params,n_estimators =1000)\n",
    "\n",
    "    eval_set  = [( x_train, y_train), ( x_val, y_val)]\n",
    "\n",
    "    clf.fit(x_train, y_train,\n",
    "            eval_set=eval_set, eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "    pred = clf.predict(x_val)\n",
    "    mse_scr = mean_squared_error(y_val, pred)\n",
    "    print (\"SCORE: %s\" % str(np.sqrt(mse_scr)))\n",
    "    return {'loss':mse_scr, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "space ={'max_depth': hp.quniform(\"x_max_depth\", 6, 16, 1),\n",
    "        'min_child_weight': hp.loguniform ('x_min_child', -0.1,3),\n",
    "        'subsample': hp.uniform ('x_subsample', 0.7, 1),\n",
    "        'gamma' : hp.uniform ('x_gamma', 0.1,0.5),\n",
    "        'colsample_bytree' : hp.uniform ('x_colsample_bytree', 0.3,1),\n",
    "        'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1),\n",
    "        'tree_method': 'gpu_hist'\n",
    "       }\n",
    "\n",
    "\n",
    "fName = \"trials_xgb_cg.p\"\n",
    "\n",
    "if os.path.exists(fName):\n",
    "    trials = pickle.load(open(fName, \"rb\"))\n",
    "    hyperparam_history = []\n",
    "    for i, loss in enumerate(trials.losses()):\n",
    "        param_vals = {k:v[i] for k,v in trials.vals.items()}\n",
    "        hyperparam_history.append((loss, param_vals))\n",
    "    hyperparam_history.sort()\n",
    "    best = hyperparam_history[0][1]\n",
    "    print (\"Parameters file loaded\")\n",
    "    print (\"BEST PARAMETERS-> \", best)\n",
    "else:#run Hyperopt optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,            \n",
    "            trials=trials)\n",
    "    print (\"BEST PARAMETERS-> \", best)\n",
    "    pickle.dump(trials, open(fName, \"wb\"))\n",
    "    print (\"Parameters dumped to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Add text features to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tf_idf_features['shop_id'].shape)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features\n",
    "train_text_features = [train_tf_idf_features[name] for name in sorted(train_tf_idf_features)]\n",
    "test_text_features = [test_tf_idf_features[name] for name in sorted(test_tf_idf_features)]\n",
    "sparse_train = hstack(train_text_features, format='csr').astype(np.float32)\n",
    "# Stack to sparse format\n",
    "sparse_train = hstack(train_text_features, format='csr').astype(np.float32)\n",
    "sparse_test = hstack(test_text_features, format='csr').astype(np.float32)\n",
    "print(sparse_train.shape)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1st limit to 30 features\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(30)\n",
    "svd = svd.fit_transform(vstack([sparse_train, sparse_test]))\n",
    "svd_train = svd[:sparse_train.shape[0]]\n",
    "svd_test = svd[sparse_train.shape[0]:]\n",
    "del svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IF NOT SVD\n",
    "train_fold = hstack([train_df] +  [sparse_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO GAIO: Create train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
